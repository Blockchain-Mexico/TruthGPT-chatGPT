# Introduction
KeysNotes:
Transformers can primarily be used in three ways, namely: (1) encoder-only (e.g., for classification), (2) decoder-only (e.g., for language modeling), and (3) encoder-decoder (e.g., for machine translation). In encoder-decoder mode, there are usually multiple multi-headed self-attention modules, including a standard self-attention in both the encoder and the decoder, along with an encoder-decoder cross-attention that allows the decoder to utilize information from the encoder

Attention between is the big deal.


Creation of engines and fast encoder decoder for the GPT models
![](/Users/astrixial/Desktop/Screenshot 2023-05-26 at 9.04.55.png)

## References:

https://www.mdpi.com/2413-4155/5/4/46