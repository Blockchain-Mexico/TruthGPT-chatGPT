# Introduction

Multi Head Attention is 




## References 

### Code

https://github.com/sooftware/attentions/blob/master/attentions.py

https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html

